{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b12934-94de-4c26-9123-5cfb85c7895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gradio to create a simple web interface for the model\n",
    "import gradio as gr\n",
    "\n",
    "# Import PIL (Python Imaging Library) to handle image loading and manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# Import PyTorch core library\n",
    "import torch\n",
    "\n",
    "# Import the neural network module from PyTorch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import commonly used image transformation tools from torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import pre-trained models from torchvision\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5498317-7ae6-4998-bee9-c63b7b46ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise fall back to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a sequence of image transformations (preprocessing steps)\n",
    "transform = transforms.Compose([\n",
    "    # Resize the input image to 256x256 pixels\n",
    "    transforms.Resize((256, 256)),\n",
    "    \n",
    "    # Convert the image to a PyTorch tensor (required for model input)\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e36f296-f93a-4733-911f-f5d6f0e59caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aarav\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aarav\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained VGG-16 model's feature extractor from torchvision\n",
    "# Set pretrained=True to use weights trained on ImageNet\n",
    "vgg = models.vgg16(pretrained=True).features\n",
    "\n",
    "# Move the model to the selected device (GPU if available, otherwise CPU)\n",
    "vgg = vgg.to(device)\n",
    "\n",
    "# Set the model to evaluation mode (disables dropout and batchnorm training behavior)\n",
    "vgg = vgg.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a11257c2-5592-449e-a2cc-25b1faf456a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which layers of the VGG network to use for content extraction\n",
    "# Layer '21' corresponds to 'relu4_2' in VGG16, commonly used for content representation\n",
    "content_layers = ['21']\n",
    "\n",
    "# Specify multiple layers of VGG to use for style extraction\n",
    "# These layer indices correspond to early and deep convolutional layers:\n",
    "# '0' = relu1_1, '5' = relu2_1, '10' = relu3_1, '19' = relu4_1, '28' = relu5_1\n",
    "# Using multiple layers helps capture both fine and coarse style patterns\n",
    "style_layers = ['0', '5', '10', '19', '28']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831206a9-6ddd-45cb-97fe-132311a7c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom PyTorch module to extract content and style features from VGG\n",
    "class VGGFeatures(nn.Module):\n",
    "    def __init__(self, model, style_layers, content_layers):\n",
    "        super(VGGFeatures, self).__init__()\n",
    "        \n",
    "        # Store the pre-trained VGG model (feature extractor)\n",
    "        self.model = model\n",
    "        \n",
    "        # List of layer names to extract style features from\n",
    "        self.style_layers = style_layers\n",
    "        \n",
    "        # List of layer names to extract content features from\n",
    "        self.content_layers = content_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Dictionaries to hold the extracted features\n",
    "        content_features = {}\n",
    "        style_features = {}\n",
    "\n",
    "        # Loop through the layers of the model sequentially\n",
    "        for name, layer in self.model._modules.items():\n",
    "            x = layer(x)  # Pass the input through the current layer\n",
    "\n",
    "            # If current layer is a content layer, store its output\n",
    "            if name in self.content_layers:\n",
    "                content_features[name] = x\n",
    "\n",
    "            # If current layer is a style layer, store its output\n",
    "            if name in self.style_layers:\n",
    "                style_features[name] = x\n",
    "\n",
    "        # Return both content and style features as dictionaries\n",
    "        return content_features, style_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47e59ca3-7cf8-478c-9154-4460864cf7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the Gram matrix of a feature map\n",
    "# Used to capture style information from an image\n",
    "def gram_matrix(tensor):\n",
    "    # Unpack tensor dimensions: batch size, channels, height, width\n",
    "    b, c, h, w = tensor.size()\n",
    "\n",
    "    # Reshape the tensor to (b * c) x (h * w)\n",
    "    # This flattens each feature map (channel) into a row vector\n",
    "    features = tensor.view(b * c, h * w)\n",
    "\n",
    "    # Compute the Gram matrix by multiplying the feature matrix with its transpose\n",
    "    # This gives a measure of how each channel correlates with others\n",
    "    G = torch.mm(features, features.t())\n",
    "\n",
    "    # Normalize the Gram matrix by the total number of elements\n",
    "    return G.div(b * c * h * w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd58443e-e844-478a-8aff-e41f00801e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and load an image for the model\n",
    "def load_image(image):\n",
    "    # Apply transformations (e.g., resize and convert to tensor),\n",
    "    # add a batch dimension (unsqueeze), and move to the selected device (CPU/GPU)\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Return the processed image tensor\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e62f632a-8a80-4297-8fbb-c098d1c4aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform neural style transfer with optional masking\n",
    "def run_style_transfer(content_img, style_img, mask_img, num_steps=300, style_weight=1e6, content_weight=1):\n",
    "    # Load and preprocess content and style images\n",
    "    content = load_image(content_img)\n",
    "    style = load_image(style_img)\n",
    "    \n",
    "    # Load and preprocess the mask image (convert to tensor and move to device)\n",
    "    mask = transform(mask_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Clone the content image as the starting point for the output image\n",
    "    input_img = content.clone().requires_grad_(True)\n",
    "\n",
    "    # Wrap the VGG model for feature extraction\n",
    "    model = VGGFeatures(vgg, style_layers, content_layers).to(device)\n",
    "\n",
    "    # Set up the optimizer (L-BFGS is good for style transfer)\n",
    "    optimizer = torch.optim.LBFGS([input_img])\n",
    "\n",
    "    # Extract target content and style features (reference features)\n",
    "    style_targets = {}\n",
    "    content_targets = {}\n",
    "    content_features, style_features = model(content)         # From content image\n",
    "    _, style_features_ref = model(style)                      # From style image\n",
    "\n",
    "    # Store the content feature outputs (as fixed targets)\n",
    "    for name in content_features:\n",
    "        content_targets[name] = content_features[name].detach()\n",
    "\n",
    "    # Store the Gram matrices for style targets\n",
    "    for name in style_features:\n",
    "        style_targets[name] = gram_matrix(style_features_ref[name].detach())\n",
    "\n",
    "    # Use a mutable object to track steps inside closure\n",
    "    run = [0]\n",
    "\n",
    "    # Optimization loop\n",
    "    while run[0] <= num_steps:\n",
    "        def closure():\n",
    "            # Clamp pixel values to be in [0, 1] range\n",
    "            input_img.data.clamp_(0, 1)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get predicted content and style features for the input image\n",
    "            content_pred, style_pred = model(input_img)\n",
    "\n",
    "            # Initialize content and style loss\n",
    "            content_loss = 0\n",
    "            style_loss = 0\n",
    "\n",
    "            # Compute content loss (MSE between input and content features)\n",
    "            for name in content_pred:\n",
    "                content_loss += content_weight * torch.nn.functional.mse_loss(\n",
    "                    content_pred[name], content_targets[name]\n",
    "                )\n",
    "\n",
    "            # Compute style loss (MSE between Gram matrices of input and style)\n",
    "            for name in style_pred:\n",
    "                G = gram_matrix(style_pred[name])\n",
    "                A = style_targets[name]\n",
    "                style_loss += style_weight * torch.nn.functional.mse_loss(G, A)\n",
    "\n",
    "            # Total loss = content + style\n",
    "            total_loss = content_loss + style_loss\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Track iterations\n",
    "            run[0] += 1\n",
    "            return total_loss\n",
    "\n",
    "        # Perform optimizer step using the closure function\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # Final clamping to ensure image values are within range\n",
    "    input_img.data.clamp_(0, 1)\n",
    "\n",
    "    # Post-process the image: convert tensor to PIL image\n",
    "    result = input_img.cpu().clone().squeeze(0)\n",
    "    result = transforms.ToPILImage()(result)\n",
    "\n",
    "    # Resize the mask and content image to match result size\n",
    "    mask_img = mask_img.convert(\"L\").resize(result.size)\n",
    "    content_img = content_img.resize(result.size)\n",
    "\n",
    "    # Blend the result and content image using the mask\n",
    "    result = Image.composite(result, content_img, mask_img)\n",
    "\n",
    "    # Return the final stylized and masked image\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc01ed-b6d4-4732-888e-99f1c58374e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a function to perform masked style transfer from file paths\n",
    "def stylize_image(content, style, mask):\n",
    "    # Load and preprocess the content image\n",
    "    content = Image.open(content).convert(\"RGB\").resize((256, 256))\n",
    "\n",
    "    # Load and preprocess the style image\n",
    "    style = Image.open(style).convert(\"RGB\").resize((256, 256))\n",
    "\n",
    "    # Load and preprocess the mask image (convert to grayscale)\n",
    "    mask = Image.open(mask).convert(\"L\").resize((256, 256))\n",
    "\n",
    "    # Perform style transfer using the previously defined function\n",
    "    output = run_style_transfer(content, style, mask)\n",
    "\n",
    "    # Return the final stylized image as a PIL object\n",
    "    return output\n",
    "\n",
    "# Create a Gradio interface for interactive style transfer\n",
    "interface = gr.Interface(\n",
    "    fn=stylize_image,  # Function to call when inputs are provided\n",
    "    inputs=[\n",
    "        gr.Image(type=\"filepath\", label=\"Content Image\"),  # User uploads content image\n",
    "        gr.Image(type=\"filepath\", label=\"Style Image\"),    # User uploads style image\n",
    "        gr.Image(type=\"filepath\", label=\"Mask Image\")      # User uploads binary mask\n",
    "    ],\n",
    "    outputs=gr.Image(type=\"pil\", label=\"Stylized Output\"),  # Display output as PIL image\n",
    "    title=\"Masked Style Transfer with VGG16\",               # Title shown on the web app\n",
    "    description=\"Upload a content image, a style image, and a binary mask to selectively stylize parts of your image.\"  # Instruction\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface (with optional debug and no public share)\n",
    "interface.launch(debug=True, share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d5589-9d74-4b35-b294-38f692480306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
